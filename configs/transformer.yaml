model:
  type: "transformer"
  params:
    sub_model:
      transformer:
        token_num: 126
        d_model: 512 
        nhead: 8
        dos_num: 64
        num_encoder_layers: 6
        num_decoder_layers: 6
        dim_feedforward: 2048
        dropout: 0.1
        activation: "gelu" # relu, relu_inplace, gelu, glu, leaky_relu

    save_best: "MAE"
    metrics_list: ["MAE", "MSE"]

    optimizer:
      transformer:
        type: "AdamW" # SGD, ASGD, Adagrad, Adamax, Adadelta, Adam, AdamW, RMSprop
        params:
          lr: 5.0e-5
          betas: [0.9, 0.98]
          weight_decay: 0.01  # 添加权重衰减
    
    lr_scheduler:
      transformer:
        sched: cosine
        epochs: &maxepoch 200
        min_lr: 1.0e-7
        warmup_lr: 1.0e-5
        warmup_epochs: 10
        lr_noise: 
        cooldown_epochs: 0



dataset:
  train:
    data_dir: "./data/train4w2023"

  test:
    data_dir: "./data/train4w2023"

  valid:
    data_dir: "./data/train4w2023"
  smear: 0



dataloader:
  num_workers: 4
  pin_memory: True
  prefetch_factor: 2
  persistent_workers: True

trainer:
  batch_size: 64
  test_batch_size: 32
  valid_batch_size: 32
  max_epoch: *maxepoch